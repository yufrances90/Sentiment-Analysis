{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                  reviews  labels\n",
       "0      russians never drop childrens toy fill explosi...       0\n",
       "1      lady tramp ii colourfully animate songs especi...       1\n",
       "2      could give movie less would certainly read rev...       0\n",
       "3      dont think ive ever give something rat one eas...       0\n",
       "4      funny bits come bill film quote zeitgeist keep...       0\n",
       "...                                                  ...     ...\n",
       "18517  western union something forget classic western...       1\n",
       "18518  movie incredible piece work explore every nook...       1\n",
       "18519  wife watch movie plan visit sicily stromboli s...       0\n",
       "18520  first watch flatliners amaze necessary feature...       1\n",
       "18521  would film good gross estimate award nominatio...       1\n",
       "\n",
       "[18522 rows x 2 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('./data/tn_reviews.csv')\n",
    "\n",
    "train_data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                 reviews  labels\n",
       "0     years since sharon stone award viewers legcros...       0\n",
       "1     someone need make car payment truly awful make...       0\n",
       "2     guidelines state comment must contain minimum ...       0\n",
       "3     movie muddle mishmash clichÃ©s recent cinema pr...       0\n",
       "4     stan laurel become smaller half alltime greate...       0\n",
       "...                                                 ...     ...\n",
       "4995  man love movie really take back kid days teach...       1\n",
       "4996  recovery incredibly move piece work handle dev...       1\n",
       "4997  take crook joint seem exceedingly difficult ta...       1\n",
       "4998  futz show preserve experimental theatre moveme...       1\n",
       "4999  mother tell recently widow mids mother two adu...       1\n",
       "\n",
       "[5000 rows x 2 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data = pd.read_csv('./data/vd_reviews.csv')\n",
    "\n",
    "valid_data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                 reviews  labels\n",
       "0     always write series complete stinkfest jim bel...       0\n",
       "1     st watch dirsteve purcell typical mary kate as...       0\n",
       "2     movie poorly write direct fell asleep minutes ...       0\n",
       "3     interest thing miryang secret sunshine actors ...       1\n",
       "4     first read berlin meer didnt expect much think...       0\n",
       "...                                                 ...     ...\n",
       "4995  kind picture john lassiter would make today we...       1\n",
       "4996  must see saw whip press screen hilarious talk ...       1\n",
       "4997  nbc ashamed wouldnt allow children see definit...       0\n",
       "4998  movie clumsy mishmash various ghoststory suspe...       0\n",
       "4999  formula movie illegitimate son rich chilenian ...       0\n",
       "\n",
       "[5000 rows x 2 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('./data/tt_reviews.csv')\n",
    "\n",
    "test_data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = train_data['reviews']\n",
    "\n",
    "all_reviews = all_reviews.append(valid_data['reviews'], ignore_index=True)\n",
    "all_reviews = all_reviews.append(test_data['reviews'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(all_reviews.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18522, 844)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = tokenizer.texts_to_sequences(train_data['reviews'].values)\n",
    "\n",
    "train_x = pad_sequences(train_x)\n",
    "\n",
    "train_y = pd.get_dummies(train_data['labels']).values\n",
    "\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 844)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x = tokenizer.texts_to_sequences(valid_data['reviews'].values)\n",
    "\n",
    "valid_x = pad_sequences(valid_x, maxlen=train_x.shape[1])\n",
    "\n",
    "valid_y = pd.get_dummies(valid_data['labels']).values\n",
    "\n",
    "valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 844)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = tokenizer.texts_to_sequences(test_data['reviews'].values)\n",
    "\n",
    "test_x = pad_sequences(test_x, maxlen=train_x.shape[1])\n",
    "\n",
    "test_y = pd.get_dummies(test_data['labels']).values\n",
    "\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 844, 128)          256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 844, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = train_x.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18522 samples, validate on 5000 samples\n",
      "Epoch 1/15\n",
      " - 464s - loss: 0.4586 - accuracy: 0.7810 - val_loss: 0.3526 - val_accuracy: 0.8554\n",
      "Epoch 2/15\n",
      " - 458s - loss: 0.3685 - accuracy: 0.8451 - val_loss: 0.3584 - val_accuracy: 0.8462\n",
      "Epoch 3/15\n",
      " - 461s - loss: 0.3274 - accuracy: 0.8658 - val_loss: 0.3526 - val_accuracy: 0.8496\n",
      "Epoch 4/15\n",
      " - 460s - loss: 0.2939 - accuracy: 0.8816 - val_loss: 0.3551 - val_accuracy: 0.8554\n",
      "Epoch 5/15\n",
      " - 462s - loss: 0.2694 - accuracy: 0.8935 - val_loss: 0.3501 - val_accuracy: 0.8540\n",
      "Epoch 6/15\n",
      " - 469s - loss: 0.2562 - accuracy: 0.8963 - val_loss: 0.3623 - val_accuracy: 0.8604\n",
      "Epoch 7/15\n",
      " - 464s - loss: 0.2412 - accuracy: 0.9034 - val_loss: 0.3630 - val_accuracy: 0.8578\n",
      "Epoch 8/15\n",
      " - 463s - loss: 0.2274 - accuracy: 0.9109 - val_loss: 0.3807 - val_accuracy: 0.8648\n",
      "Epoch 9/15\n",
      " - 471s - loss: 0.2155 - accuracy: 0.9142 - val_loss: 0.3923 - val_accuracy: 0.8598\n",
      "Epoch 10/15\n",
      " - 470s - loss: 0.1910 - accuracy: 0.9265 - val_loss: 0.4076 - val_accuracy: 0.8514\n",
      "Epoch 11/15\n",
      " - 475s - loss: 0.1776 - accuracy: 0.9324 - val_loss: 0.4151 - val_accuracy: 0.8584\n",
      "Epoch 12/15\n",
      " - 479s - loss: 0.1658 - accuracy: 0.9355 - val_loss: 0.4350 - val_accuracy: 0.8478\n",
      "Epoch 13/15\n",
      " - 482s - loss: 0.1581 - accuracy: 0.9413 - val_loss: 0.4553 - val_accuracy: 0.8546\n",
      "Epoch 14/15\n",
      " - 478s - loss: 0.1431 - accuracy: 0.9455 - val_loss: 0.4487 - val_accuracy: 0.8538\n",
      "Epoch 15/15\n",
      " - 478s - loss: 0.1362 - accuracy: 0.9482 - val_loss: 0.4810 - val_accuracy: 0.8528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb177e06b10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(\n",
    "    train_x, train_y, \n",
    "    epochs = 15, \n",
    "    batch_size = batch_size, \n",
    "    verbose = 2,\n",
    "    validation_data=(valid_x, valid_y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.46003970744609834, 0.850600004196167]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_x, test_y, verbose = 2, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"output/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
